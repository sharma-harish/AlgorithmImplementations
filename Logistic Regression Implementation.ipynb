{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Linear Regression but it does not work well with outliers. So we use Logistic Regressiin which very similar to Linear Regression. In Logistic Regression, the decision boundary is the property of hypothesis, not of the data. Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionImplemented:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        self.error = np.float('inf')\n",
    "        self.costs = np.array([])\n",
    "    \n",
    "    def fit(self, X, y, alpha, iterations = 100, iter_result = False):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        r, c = X.shape\n",
    "        #initial values of parameters\n",
    "        theta = np.ones((c, 1))\n",
    "        for i in range(iterations):\n",
    "            prod = X @ theta\n",
    "            y_pred = 1 / (1 + np.exp(-prod))\n",
    "            y_pred[y_pred >= 0.5] = 1\n",
    "            y_pred[y_pred < 0.5] = 0\n",
    "            c = self.cost(y_pred, y)\n",
    "            if iter_result:\n",
    "                print('Iteration ' + str(i) + ': ' + str(c))\n",
    "            self.costs = np.append(self.costs, c)\n",
    "            if (c < self.error):\n",
    "                self.error = c\n",
    "                self.theta = theta\n",
    "            theta = self.gradientDescent(X, y_pred, y, theta, alpha)\n",
    "            \n",
    "    def cost(self, y_pred, y):\n",
    "        class1_cost = y*np.log(y_pred)\n",
    "        class2_cost = (1 - y)*np.log(1 - y_pred)\n",
    "        cost = - (sum(class1_cost) + sum(class2_cost)) / len(y)\n",
    "        return cost\n",
    "    \n",
    "    def gradientDescent(self, X, y_pred, y, theta, alpha):\n",
    "        diff = X.T @ (y_pred - y)\n",
    "        theta = theta - (alpha / y.shape[0]) * diff\n",
    "        return theta\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.error, self.theta\n",
    "    \n",
    "    def plotCost(self):\n",
    "        plt.plot(self.costs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
